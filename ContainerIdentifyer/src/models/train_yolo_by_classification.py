import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from PIL import Image
import glob
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# åˆ†ç±»è®­ç»ƒYOLO

class ContainerDamageDataset(Dataset):
    """é›†è£…ç®±ç ´æŸåˆ†ç±»æ•°æ®é›†"""

    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert('RGB')
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label


class DamageClassifier:
    def __init__(self, num_classes=2):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        self.model = self._create_model(num_classes)
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)

        self.transform = self._get_transforms()

    def _create_model(self, num_classes):
        """åˆ›å»ºåˆ†ç±»æ¨¡å‹"""
        model = models.resnet50(pretrained=True)
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        return model.to(self.device)

    def _get_transforms(self):
        """æ•°æ®é¢„å¤„ç†"""
        train_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        val_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        return {'train': train_transform, 'val': val_transform}

    def prepare_data(self, image_folder, label_folder):
        """å‡†å¤‡åˆ†ç±»æ•°æ®"""
        print("ğŸ“ å‡†å¤‡åˆ†ç±»æ•°æ®é›†...")

        # è·å–æ‰€æœ‰å›¾ç‰‡
        image_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            image_files.extend(glob.glob(os.path.join(image_folder, ext)))

        # æ ¹æ®æ ‡æ³¨æ–‡ä»¶åˆ¤æ–­æ˜¯å¦æœ‰ç ´æŸ
        image_paths = []
        labels = []

        for img_path in image_files:
            img_name = os.path.basename(img_path)
            label_file = os.path.join(label_folder, os.path.splitext(img_name)[0] + '.txt')

            # å¦‚æœæœ‰æ ‡æ³¨æ–‡ä»¶ï¼Œè¯´æ˜æœ‰ç ´æŸï¼ˆç±»åˆ«1ï¼‰ï¼Œå¦åˆ™æ— ç ´æŸï¼ˆç±»åˆ«0ï¼‰
            if os.path.exists(label_file):
                image_paths.append(img_path)
                labels.append(1)  # æœ‰ç ´æŸ
            else:
                image_paths.append(img_path)
                labels.append(0)  # æ— ç ´æŸ

        print(f"æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  æ€»å›¾ç‰‡æ•°: {len(image_paths)}")
        print(f"  æ— ç ´æŸå›¾ç‰‡: {labels.count(0)}")
        print(f"  æœ‰ç ´æŸå›¾ç‰‡: {labels.count(1)}")

        return image_paths, labels

    def train(self, train_loader, val_loader, epochs=10):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒåˆ†ç±»æ¨¡å‹...")

        best_accuracy = 0
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            for images, labels in train_loader:
                images, labels = images.to(self.device), labels.to(self.device)

                self.optimizer.zero_grad()
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()

                train_loss += loss.item()

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            all_preds = []
            all_labels = []

            with torch.no_grad():
                for images, labels in val_loader:
                    images, labels = images.to(self.device), labels.to(self.device)
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
                    val_loss += loss.item()

                    _, preds = torch.max(outputs, 1)
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

            accuracy = accuracy_score(all_labels, all_preds)

            print(f'Epoch {epoch + 1}/{epochs}:')
            print(f'  è®­ç»ƒæŸå¤±: {train_loss / len(train_loader):.4f}')
            print(f'  éªŒè¯æŸå¤±: {val_loss / len(val_loader):.4f}')
            print(f'  éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f}')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                torch.save(self.model.state_dict(), 'best_classifier.pth')
                print(f'  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {accuracy:.4f}')


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("     é›†è£…ç®±ç ´æŸåˆ†ç±»æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)

    # é…ç½®è·¯å¾„
    image_folder = r"D:\ContainerIdentifyer\data\images"
    label_folder = r"D:\ContainerIdentifyer\data\labels"

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = DamageClassifier(num_classes=2)

    # å‡†å¤‡æ•°æ®
    image_paths, labels = classifier.prepare_data(image_folder, label_folder)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    from sklearn.model_selection import train_test_split
    train_paths, val_paths, train_labels, val_labels = train_test_split(
        image_paths, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = ContainerDamageDataset(train_paths, train_labels, classifier.transform['train'])
    val_dataset = ContainerDamageDataset(val_paths, val_labels, classifier.transform['val'])

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # å¼€å§‹è®­ç»ƒ
    classifier.train(train_loader, val_loader, epochs=10)


if __name__ == "__main__":
    main()